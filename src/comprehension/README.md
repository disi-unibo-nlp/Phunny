# Comprehension

We divide this task into two subtasks based on the type of input:  

- **Coherent:** The model receives a valid pun with a well-formed punchline.  
- **Misleading:** The model is given a nonsensical pun, generated by altering a coherent one.  

Misleading puns test whether LLMs forcefully interpret non-puns as real puns, fabricating justifications even when none exist.  

To construct misleading puns, we modify each original pun's subject (**X**) in two ways:  

- **Semantically similar swap:** We replace **X** with a term of high cosine similarity (e.g., *"cat"* → *"pet"*).  
  *We compute word embeddings using the `all-MiniLM-L6-v2` model from Hugging Face.*  
- **Semantically dissimilar swap:** We substitute **X** with the most unrelated term (e.g., *"cat"* → *"bus"*), making the pun illogical.  

We evaluate models on **1,050 puns**, consisting of **350 coherent** and **700 misleading** samples.  

Data used for this task can be found in **[`data/phunny_comprehension.jsonl`](data/phunny_comprehension.jsonl)**.  
