# Comprehension

We divide this task into two subtasks based on the type of input:  

- **Coherent:** The model receives a valid pun with a well-formed punchline.  
- **Misleading:** The model is given a nonsensical pun, generated by altering a coherent one.  

Misleading puns test whether LLMs forcefully interpret non-puns as real puns, fabricating justifications even when none exist.  

To construct misleading puns, we modify each original pun's subject (**X**) in two ways:  

- **Semantically similar swap:** We replace **X** with a term of high cosine similarity (e.g., *"cat"* → *"pet"*).  
  *We compute word embeddings using the `all-MiniLM-L6-v2` model from Hugging Face.*  
- **Semantically dissimilar swap:** We substitute **X** with the most unrelated term (e.g., *"cat"* → *"bus"*), making the pun illogical.  

We evaluate models on **1,050 puns**, consisting of **350 coherent** and **700 misleading** samples.  

Data used for this task can be found in **[`data/phunny_comprehension.jsonl`](data/phunny_comprehension.jsonl)**.  

# Experiments

## Prompt
```
{english_pun}

Did you get it? Possible answers:
- Yes, for exactly two reasons. First, ... Second, ...
- No, because ...

Don't add further information.
```

## Set Modality
- For **Coherent Comprehension**, set `mode="logical"` and ignore the `illogical_selection` parameter.  
- For **Coherent Comprehension**, set `mode="illogical"` and setup the `illogical_selection` parameter as follows:  
  - To use a **semantically similar swap**, set `illogical_selection="most_similar"`.  
  - To use a **semantically dissimilar swap**, set `illogical_selection="least_similar"`.  

## vLLM Inference  

Below is an example Bash script to run inference for the **Misleading Comprehension** task with a **semantically similar swap** (`"most_similar"`) using vLLM.  

```bash
#!/bin/bash

python3 -m src.comprehension.run_vllm \
    --model_name "meta-llama/Llama-3.1-8B-Instruct" \
    --input_data "data/Phunny_comprehension.jsonl" \
    --out_dir "./out" \
    --max_samples -1 \
    --start_idx 0 \
    --batch_size 8 \
    --cache_dir None \
    --max_model_len 1024 \
    --max_new_tokens None \
    --top_p 1.0 \
    --n_out_sequences 1 \
    --temperature 0.0 \
    --mode "illogical" \
    --illogical_selection "most_similar" \
    --n_gpus 1
```

## OpenAI Batch Inference
Below is an example Bash script to run inference for the **Misleading Comprehension** task with a **semantically similar swap** (`"most_similar"`) using the OpenAI Batch API. 

```bash
#!/bin/bash

python3 -m src.comprehension.run_openai_batch \
    --model_name "gpt-4o-mini-2024-07-18" \
    --input_data "data/Phunny_cohmprension.jsonl" \
    --max_samples -1 \
    --start_idx 0 \
    --top_p 1.0 \
    --n_sampling 1 \
    --temperature 0.0 \
    --n_shots "5" \
    --mode "illogical" \
    --illogical_selection "most_similar"
```

## Gemini Inference

Below is an example Bash script to run inference for the **Misleading Comprehension** task with a **semantically similar swap** (`"most_similar"`) using the Gemini API.

```bash
#!/bin/bash

python3 -m src.comprehension.run_gemini \
    --model_name "gemini-2.0-flash-thinking-exp" \
    --max_samples -1 \
    --input_data "data/Phunny_cohmprension.jsonl" \
    --start_idx 0 \
    --top_p 1.0 \
    --top_k 200 \
    --temperature 0 \
    --mode "logical" \
    --illogical_selection "most_similar"
```

# Evaluation
The evaluation is coducted by employing Gemini-1.5-flash as judge. The prompt adopted is the following:
```
Determine whether the given explanations are equivalent. This means that the predicted answer should match both of the two reasons given by the gold answer.

Gold explanation:
There are two reasons:

- First, '{answer}' starts with '{prefix}' or '{answer}' sounds like {prefix} or '{answer}' is a play of words with '{prefix}'.
- Second, '{answer}' means '{definition}' or '{answer}' is highly associated to '{definition}'.

Predicted explanation:
{item['answer'].strip()}

Question: 
Are the two explanations equivalent?

Explain briefly your decision and then answer with "yes" or "no" prefixed by "Answer:".
```

Below is an example Bash script to run evaluation with gemini-1.5-flash on completions made by gpt-4o
```bash
#!/bin/bash

python3 -m src.comprehension.eval_gemini \
    --judge_name "gemini-1.5-flash" \
    --model_name "gpt-4o-2024-08-06" \
    --input_data "out/gpt-4o-2024-08-06/illogical_least_similar/2025-02-10_15-47-55/comprehension.jsonl" \
    --max_samples -1 \
    --start_idx 0 \
    --top_p 1.0 \
    --temperature 0
```
