# Comprehension

We divide this task into two subtasks based on the type of input:  

- **Coherent:** The model receives a valid pun with a well-formed punchline.  
- **Misleading:** The model is given a nonsensical pun, generated by altering a coherent one.  

Misleading puns test whether LLMs forcefully interpret non-puns as real puns, fabricating justifications even when none exist.  

To construct misleading puns, we modify each original pun's subject (**X**) in two ways:  

- **Semantically similar swap:** We replace **X** with a term of high cosine similarity (e.g., *"cat"* → *"pet"*).  
  *We compute word embeddings using the `all-MiniLM-L6-v2` model from Hugging Face.*  
- **Semantically dissimilar swap:** We substitute **X** with the most unrelated term (e.g., *"cat"* → *"bus"*), making the pun illogical.  

We evaluate models on **1,050 puns**, consisting of **350 coherent** and **700 misleading** samples.  

Data used for this task can be found in **[`data/phunny_comprehension.jsonl`](data/phunny_comprehension.jsonl)**.  

# Run Experiments

## vLLM Inference

Example bash script to run for Misleading comprehension and semantically similar swap ("most_similar"). Note: to use demntically dissimilar swap just set illogical_selection param to "least_similar". Set instead mode to "logical" and ignore the param illogical_selection for Coehrent comprension. 

```bash
#!/bin/bash

python3 -m src.comprehension.run_vllm \
    --model_name "meta-llama/Llama-3.1-8B-Instruct" \
    --input_data "data/Phunny_comprehension.jsonl" \
    --out_dir "./out" \
    --max_samples -1 \
    --start_idx 0 \
    --batch_size 8 \
    --cache_dir None \
    --max_model_len 1024 \
    --max_new_tokens None \
    --top_p 1.0 \
    --n_out_sequences 1 \
    --temperature 0.0 \
    --mode "illogical" \
    --illogical_selection "most_similar" \
    --n_gpus 1
```

## OpenAI Batch Inference
Example bash script to run for illogical comprehension and semantically similar swap ("most_similar") by using the OpenAI Batch API. Note: to use demntically dissimilar swap just set illogical_selection param to "least_similar". Set instead mode to "logical" and ignore the param illogical_selection for Coehrent comprension. 

```bash
#!/bin/bash

python3 -m src.comprehension.run_vllm \
    --model_name "gpt-4o-mini-2024-07-18" \
    --input_data "data/Phunny_cohmprension.jsonl" \
    --max_samples -1 \
    --start_idx 0 \
    --top_p 1.0 \
    --n_sampling 1 \
    --temperature 0.0 \
    --n_shots "5" \
    --mode "illogical" \
    --illogical_selection "most_similar"
```

## Gemini Inference
